{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "This lab will guide you through the key concepts of CV, ML and affective computing needed for the course. This lab aims to give you all the tools you might need for the affective recognition part of the final project. Many of these tools are alternative to each other so in the final project you will have the option to choose the one that best fits your needs/expertise and/or compare among them. This lab is *entirely optional* but we **strongly recommend** you go at least through the filled parts. The solution to the exercises will be provided after the last help session.\n",
    "\n",
    "\n",
    "Suggested reading structure:\n",
    "* Full (recommended): begin from the top and go to the bottom. You can skip parts you are already familiar with.\n",
    "* Essential: jump to S1.3, continue until the end of S1.4; jump to S2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1: Image processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utilities\n",
    "Here, we do the imports for S1. We also apply some patches to OpenFace to fix some issues and improve usability for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, Video\n",
    "from openface.face_detection import FaceDetector\n",
    "\n",
    "# The next block fixes a bug that causes crashes on Unix-based systems.\n",
    "# You should apply this patch before importing \"LandmarkDetector\" from openface.\n",
    "\n",
    "# --- begin of patch ---\n",
    "# Patching LandmarkDetector hardcoded log folder\n",
    "from openface.STAR.demo import Alignment\n",
    "\n",
    "# Access the utility module through Alignment.__init__'s globals\n",
    "utility = Alignment.__init__.__globals__['utility']\n",
    "\n",
    "# Only patch once to avoid recursion\n",
    "if getattr(utility.set_environment, '__name__', '') != 'patched_set_environment':\n",
    "    original_set_environment = utility.set_environment\n",
    "\n",
    "    def patched_set_environment(config):\n",
    "        result = original_set_environment(config)\n",
    "        config.log_dir = '.'\n",
    "        return result\n",
    "\n",
    "    utility.set_environment = patched_set_environment\n",
    "# --- end of patch ---\n",
    "\n",
    "from openface.landmark_detection import LandmarkDetector\n",
    "from openface.multitask_model import MultitaskPredictor\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import os\n",
    "\n",
    "\n",
    "# The next patch overrides a function in FaceDetector which expects a filename as input to also accept numpy arrays\n",
    "# This will be useful later when we want to pass webcam frames directly without saving to disk\n",
    "\n",
    "# --- begin of patch ---\n",
    "def override_preprocess_image(self, image_path: Union[str, np.ndarray], resize: float = 1.0):\n",
    "    if isinstance(image_path, (str, os.PathLike)):\n",
    "        img_raw = cv2.imread(str(image_path), cv2.IMREAD_COLOR)  # BGR, 3 channels\n",
    "        if img_raw is None:\n",
    "            raise ValueError(f\"Failed to read image from path: {image_path}\")\n",
    "    elif isinstance(image_path, np.ndarray):\n",
    "        img_raw = image_path\n",
    "    else:\n",
    "        raise TypeError(\"image_path must be a str/Path-like path or a numpy.ndarray (BGR frame).\")\n",
    "\n",
    "    if img_raw.ndim == 2:\n",
    "        # Grayscale -> BGR\n",
    "        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_GRAY2BGR)\n",
    "    elif img_raw.ndim == 3:\n",
    "        if img_raw.shape[2] == 4:\n",
    "            # BGRA -> BGR\n",
    "            img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGRA2BGR)\n",
    "        elif img_raw.shape[2] != 3:\n",
    "            raise ValueError(f\"Unsupported channel count: {img_raw.shape[2]} (expected 1, 3, or 4)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image shape {img_raw.shape}; expected HxW or HxWxC.\")\n",
    "\n",
    "    # --- Preprocess as in original code\n",
    "    img = img_raw.astype(np.float32, copy=False)\n",
    "    if resize != 1.0:\n",
    "        img = cv2.resize(img, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Mean subtraction in BGR (matching many Caffe-style models)\n",
    "    img -= (104.0, 117.0, 123.0)\n",
    "\n",
    "    # Ensure contiguous before transpose (safer with slices or unusual strides)\n",
    "    img = np.ascontiguousarray(img.transpose(2, 0, 1))  # (C, H, W)\n",
    "\n",
    "    img = torch.from_numpy(img).unsqueeze(0).to(self.device)  # (1, C, H, W)\n",
    "\n",
    "    return img, img_raw\n",
    "\n",
    "FaceDetector.preprocess_image = override_preprocess_image\n",
    "# --- end of patch ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1.1: Image Manipulation\n",
    "\n",
    "We can load images as `ndarray`s shaped `(height, width, channels)` using `iio.imread()`, and display them with `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seagull = iio.imread(\"seagull.jpg\") # Copyright Yuqiong Wang, 2023. Used with permission from the author.\n",
    "plt.imshow(seagull)\n",
    "type(seagull), seagull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the type is `uint8`: integers in range 0 ... 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seagull.dtype, seagull.min(), seagull.mean(), seagull.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the picture is represented as an array, we can easily manipulate it. E.g., we can use *index slicing* to crop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout = seagull[190:640, 525:975]\n",
    "plt.imshow(cutout)\n",
    "cutout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do an extreme crop, we can see the individual pixels that form the image. In RGB, each pixel is represented by 3 values: Red, Green, and Blue intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_cutout = seagull[230:255, 595:620]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(extreme_cutout)\n",
    "extreme_cutout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that our array stores the colors as RGB, let's set R to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_red = cutout.copy()\n",
    "no_red[:, :, 0] = 0\n",
    "plt.imshow(no_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can invert colors with some math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = 255 - cutout\n",
    "plt.imshow(inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iio.imwrite()` allows us to save the image to a variety of formats. In this case, `JPG`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iio.imwrite(\"inverse.jpg\", inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a PNG with transparency now. In this case, the channels are `RGBA`: Red, Green, Blue, Alpha.\n",
    "* `A = 0` means fully transparent\n",
    "* `A = 255` means fully opaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu_logo = iio.imread(\"uu_logo.png\") # Trademark owned by Uppsala University.\n",
    "plt.imshow(uu_logo)\n",
    "uu_logo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PyPlot adds a white background, it's not so obvious that we have transparency. Let's make everything fully opaque to reveal the hidden RGB colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alpha = uu_logo.copy()\n",
    "full_alpha[:, :, 3] = 255\n",
    "plt.imshow(full_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get tired of the PyPlot numbers, in a Jupyter notebook you can use `IPython.display.Image`. It even respects transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"uu_logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1.2: Video Manipulation\n",
    "\n",
    "There is no good way to display videos using PyPlot, but we can use `IPython.display.Video` to embed a video in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"cats.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same command `iio.imread()` we used for images can also be used for videos. In this case, the array is shaped `(frames, height, width, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"cats\": copyright Marc Fraile, 2023.\n",
    "cats = iio.imread(\"cats.mp4\")\n",
    "cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the video as a sequence of frames discards some important information, like the framerate. We can recover this information (and other *metadata*) using `iio.immeta()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_meta = iio.immeta(\"cats.mp4\")\n",
    "cats_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can display individual frames just like we display images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cats[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this first block, we will practice one of the oldest and simplest tricks in Image Analysis: **color thresholding**. We will try to isolate a brightly colored object in an image by checking color values.\n",
    "\n",
    "Exercise 1 below: Load file \"toy_knife.jpg\", find color thresholds to isolate some objects, for example a toy or a hand. \n",
    "Exercise 2 below: Same as Exercise 1, but try to do this in HSV image representation. \n",
    "\n",
    "### Hints\n",
    "\n",
    "* `np.zeros_like(image, shape=...)`\n",
    "* You can use comparisons to index into an `ndarray`.\n",
    "* You can copy most code from Exercise 1 into Exercise 2.\n",
    "* `cv2.cvtColor()`\n",
    "* `cv2` also has loading and saving functions `cv2.imread()` and `cv2.imwrite()`, but they expect BGR and don't work with videos.\n",
    "* Use the internet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# ---------------------------------------------------------------- #\n",
    "# (1) Load \"toy_knife.jpg\"\n",
    "# (2) Make a copy that has half the values of the original.\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# ---------------------------------------------------------------- #\n",
    "\n",
    "cv2.namedWindow(\"Color Picker (Esc to Exit)\")\n",
    "\n",
    "min_values = [ 0, 0, 0 ]\n",
    "max_values = [ 255, 255, 255 ]\n",
    "\n",
    "def check_trackbar():\n",
    "    min_values[0] = cv2.getTrackbarPos(\"R min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[0] = cv2.getTrackbarPos(\"R max\", \"Color Picker (Esc to Exit)\")\n",
    "    min_values[1] = cv2.getTrackbarPos(\"G min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[1] = cv2.getTrackbarPos(\"G max\", \"Color Picker (Esc to Exit)\")\n",
    "    min_values[2] = cv2.getTrackbarPos(\"B min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[2] = cv2.getTrackbarPos(\"B max\", \"Color Picker (Esc to Exit)\")\n",
    "\n",
    "cv2.createTrackbar(\"R min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"R max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"G min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"G max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"B min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"B max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "\n",
    "while True:\n",
    "    # ---------------------------------------------------------------- #\n",
    "    # (3) Create an image that is twice as wide as the original.\n",
    "    #     The left side must contain the pixels from the original image,\n",
    "    #     and the right pixels must contain the pixels from the half-intensity image.\n",
    "    # (4) \"Mask\" the wide image you just created:\n",
    "    #      For each pixel, check if it's within the expected range (min_R <= R <= max_R, etc.),\n",
    "    #      and set it to 0 if it's OUTSIDE the range.\n",
    "    # (5) Display the masked image in the CV2 window \"Color Picker\".\n",
    "    \n",
    "    # Your code here:\n",
    "\n",
    "    # ---------------------------------------------------------------- #\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27: # ESC pressed\n",
    "        break\n",
    "\n",
    "# ---------------------------------------------------------------- #\n",
    "# (6) Save the masked image to \"masked_rgb.jpg\".\n",
    "# (7) Print the `min_values` and `max_values`.\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# ---------------------------------------------------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "# ---------------------------------------------------------------- #\n",
    "# (1) Load \"toy_knife.jpg\"\n",
    "# (2) Make a copy that has half the values of the original.\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# ---------------------------------------------------------------- #\n",
    "\n",
    "cv2.namedWindow(\"Color Picker (Esc to Exit)\")\n",
    "\n",
    "min_values = [ 0, 0, 0 ]\n",
    "max_values = [ 255, 255, 255 ]\n",
    "\n",
    "def check_trackbar():\n",
    "    min_values[0] = cv2.getTrackbarPos(\"H min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[0] = cv2.getTrackbarPos(\"H max\", \"Color Picker (Esc to Exit)\")\n",
    "    min_values[1] = cv2.getTrackbarPos(\"S min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[1] = cv2.getTrackbarPos(\"S max\", \"Color Picker (Esc to Exit)\")\n",
    "    min_values[2] = cv2.getTrackbarPos(\"V min\", \"Color Picker (Esc to Exit)\")\n",
    "    max_values[2] = cv2.getTrackbarPos(\"V max\", \"Color Picker (Esc to Exit)\")\n",
    "\n",
    "cv2.createTrackbar(\"H min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"H max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"S min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"S max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"V min\", \"Color Picker (Esc to Exit)\",   0, 255, lambda x: None)\n",
    "cv2.createTrackbar(\"V max\", \"Color Picker (Esc to Exit)\", 255, 255, lambda x: None)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # ---------------------------------------------------------------- #\n",
    "    # (3) Create an image that is twice as wide as the original.\n",
    "    #     The left side must contain the pixels from the original image,\n",
    "    #     and the right pixels must contain the pixels from the half-intensity image.\n",
    "    # (4) Convert the wide image to HSV using OpenCV.\n",
    "    # (5) Mask the wide image like you did in the RGB exercise,\n",
    "    # BUT this time use the HSV values for masking.\n",
    "    # (6) Display the masked image in the CV2 window \"Color Picker\".\n",
    "    \n",
    "    # Your code here:\n",
    "\n",
    "    # ---------------------------------------------------------------- #\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27: # ESC pressed\n",
    "        break\n",
    "\n",
    "# ---------------------------------------------------------------- #\n",
    "# (7) Save the masked image to \"masked_rgb.jpg\".\n",
    "# (8) Print the `min_values` and `max_values`.\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# ---------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1.3: Webcam Access\n",
    "\n",
    "If you look up online how to do any image processing task in Python, you will be told to use [OpenCV](https://opencv.org/). This is an old C++ library with a clunky Python interface, and has plenty of downsides, but it's hard to beat it in number of features or speed of execution.\n",
    "\n",
    "We will use OpenCV for real-time webcam access. To smooth over its usage in notebooks, we will also use `opencv_jupyter_ui`.\n",
    "\n",
    "We have written an utility to take snapshots and another to take videos using OpenCV. You can find the code in the module `camera_widget`. Let's test them out, starting with the picture-taking app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            print(\"Camera read failed.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "        # Wait 1 ms and check for ESC (27)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check the video-taking solution next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_write_text(img: np.ndarray, text: str, position: tuple[int, int],\n",
    "                   color: tuple[int, int, int] = (255, 255, 255)) -> None:\n",
    "    \"\"\"Write text with a black outline for readability.\"\"\"\n",
    "    cv2.putText(img, text, position, cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 0), 6, cv2.LINE_AA)\n",
    "    cv2.putText(img, text, position, cv2.FONT_HERSHEY_PLAIN, 2, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def cv2_vid() -> tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Webcam capture using native OpenCV window.\n",
    "    Controls:\n",
    "      - SPACE: start/stop recording\n",
    "      - ESC:   exit\n",
    "    Returns: (fps, video) where video is RGB np.ndarray with shape (T, H, W, 3)\n",
    "             or an empty 0-d array if nothing recorded.\n",
    "    \"\"\"\n",
    "    cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # CAP_DSHOW helps on Windows\n",
    "    if not cam.isOpened():\n",
    "        raise RuntimeError(\"Could not open default camera (index 0).\")\n",
    "\n",
    "    fps = cam.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "\n",
    "    video: list[np.ndarray] = []\n",
    "    playback_idx = 0\n",
    "    last_recording = False\n",
    "    recording = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cam.read()\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"OpenCV failed to read a frame.\")\n",
    "\n",
    "            h, w, c = frame.shape\n",
    "            display = np.zeros((h, w * 2, c), dtype=frame.dtype)\n",
    "            display[:, :w, :] = frame\n",
    "\n",
    "            if video:\n",
    "                playback_idx = (playback_idx + 1) % len(video)\n",
    "                display[:, w:, :] = video[playback_idx]\n",
    "            else:\n",
    "                cv2_write_text(display, \"No playback yet\", (int(1.05 * w), int(0.1 * h)), (200, 200, 200))\n",
    "\n",
    "            if recording:\n",
    "                if not last_recording:\n",
    "                    video = []  # start a fresh take on first frame after toggling on\n",
    "                cv2_write_text(display, \"RECORDING. Press SPACE to stop.\",\n",
    "                               (int(1.05 * w), int(0.8 * h)), (127, 127, 255))\n",
    "                video.append(frame.copy())\n",
    "            else:\n",
    "                cv2_write_text(display, \"STOPPED. Press SPACE to start.\",\n",
    "                               (int(1.05 * w), int(0.8 * h)), (127, 127, 127))\n",
    "\n",
    "            cv2_write_text(display, \"Press ESC to exit\", (int(1.05 * w), int(0.9 * h)))\n",
    "\n",
    "            cv2.imshow(\"vid\", display)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 27:       # ESC\n",
    "                break\n",
    "            elif key == 32:     # SPACE\n",
    "                recording = not recording\n",
    "\n",
    "            last_recording = recording\n",
    "    finally:\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Convert to RGB for downstream use\n",
    "    if not video:\n",
    "        converted = np.empty(shape=(), dtype=np.uint8)\n",
    "    else:\n",
    "        converted = np.stack([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in video], axis=0)\n",
    "\n",
    "    return float(fps), converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps, vid = cv2_vid()\n",
    "fps, vid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the video, let's save it to a local file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iio.imwrite(\"webcam_video.mp4\", vid, fps=fps)\n",
    "Video(\"webcam_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get creative and output the channels separately. Note that OpenCV does not follow the standard RGB convention, using BGR instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # CAP_DSHOW helps on Windows startup\n",
    "\n",
    "if not cam.isOpened():\n",
    "    raise RuntimeError(\"Could not open default camera (index 0).\")\n",
    "\n",
    "while True:\n",
    "    ok, in_frame = cam.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    h, w, c = in_frame.shape\n",
    "    out_frame = np.zeros((h * 2, w * 2, c), dtype=np.uint8)\n",
    "\n",
    "    # Top-left: original image\n",
    "    out_frame[:h, :w, :] = in_frame\n",
    "\n",
    "    # Top-right: red channel only\n",
    "    out_frame[:h, w:, 0] = in_frame[:, :, 0]\n",
    "\n",
    "    # Bottom-left: green channel only\n",
    "    out_frame[h:, :w, 1] = in_frame[:, :, 1]\n",
    "\n",
    "    # Bottom-right: blue channel only\n",
    "    out_frame[h:, w:, 2] = in_frame[:, :, 2]\n",
    "\n",
    "    cv2.imshow(\"Video (ESC to exit)\", out_frame)\n",
    "\n",
    "    # ESC key closes\n",
    "    if cv2.waitKey(10) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1.4: Feature Extraction\n",
    "\n",
    "In this course, we are interested in features related to the expression of emotion. We will focus in facial features: the position of the face, the activation of different muscles used to express emotion, or even the expressed emotion itself. OpenFace is a modern Python library that allows us to easily find facial landmarks locations. Using this locations, one may derive emotions analytically (for example to see of mouth corners are above mouth center if a person smiles) or use some kind of machnie learning algorithms to detect human emotions. \n",
    "\n",
    "Let's load a detector and test it on the faces of the TAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the detector packages several models with different functions: finding faces in a picture, detecting key points (landmarks) in each face, deducing facial muscle activations (AUs), detecting emotion...\n",
    "\n",
    "We can pass a filename to `detector.detect_image()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"lux.jpg\"\n",
    "device = \"cpu\"  # or \"cuda\" to make it run faster if you have a compatible GPU; you can also try \"mps\" on MacOS with Apple Silicon\n",
    "\n",
    "detector = FaceDetector(model_path=\"./weights/Alignment_RetinaFace.pth\", device=device)\n",
    "\n",
    "# Run detection (the API returns (img, detections)); we only need detections\n",
    "_, dets = detector.get_face(image_path)\n",
    "if len(dets)==0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    vis = img_bgr.copy()\n",
    "\n",
    "    boxes = []\n",
    "    for i, det in enumerate(dets):\n",
    "        # Expected RetinaFace-style: [x1,y1,x2,y2,score, lmk(10)?]\n",
    "        x1, y1, x2, y2 = map(int, det[:4])\n",
    "        score = float(det[4]) if len(det) > 4 else None\n",
    "        boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "        # draw box\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "        # draw 5 landmarks if present\n",
    "        if len(det) >= 15:\n",
    "            lm = det[5:15].reshape(-1,2)\n",
    "            for (lx, ly) in lm.astype(int):\n",
    "                cv2.circle(vis, (lx, ly), 2, (255,0,0), -1)\n",
    "\n",
    "        # label with confidence\n",
    "        if score is not None:\n",
    "            cv2.putText(vis, f\"{score:.3f}\", (x1, max(0,y1-6)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(vis, f\"{score:.3f}\", (x1, max(0,y1-6)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Show in notebook (RGB)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Faces: {len(dets)}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print details\n",
    "    print(\"Bounding boxes [x1,y1,x2,y2]:\", boxes)\n",
    "    for i, det in enumerate(dets):\n",
    "        info = {\"score\": float(det[4]) if len(det) > 4 else None}\n",
    "        if len(det) >= 15:\n",
    "            info[\"landmarks_5\"] = det[5:15].reshape(-1,2).tolist()\n",
    "        print(f\"Face {i+1}:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that FaceDetector.get_face() returns not only face location, but also can find multiple facial landmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Alessio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"alessio.jpg\"\n",
    "device = \"cpu\"  # or \"cuda\" to make it run faster if you have a compatible GPU; you can also try \"mps\" on MacOS with Apple Silicon\n",
    "\n",
    "detector = FaceDetector(model_path=\"./weights/Alignment_RetinaFace.pth\", device=device)\n",
    "\n",
    "# Run detection (the API returns (img, detections)); we only need detections\n",
    "_, dets = detector.get_face(image_path)\n",
    "if len(dets)==0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    vis = img_bgr.copy()\n",
    "\n",
    "    boxes = []\n",
    "    for i, det in enumerate(dets):\n",
    "        # Expected RetinaFace-style: [x1,y1,x2,y2,score, lmk(10)?]\n",
    "        x1, y1, x2, y2 = map(int, det[:4])\n",
    "        score = float(det[4]) if len(det) > 4 else None\n",
    "        boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "        # draw box\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "        # draw 5 landmarks if present\n",
    "        if len(det) >= 15:\n",
    "            lm = det[5:15].reshape(-1,2)\n",
    "            for (lx, ly) in lm.astype(int):\n",
    "                cv2.circle(vis, (lx, ly), 2, (255,0,0), -1)\n",
    "\n",
    "        # label with confidence\n",
    "        if score is not None:\n",
    "            cv2.putText(vis, f\"{score:.3f}\", (x1, max(0,y1-6)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(vis, f\"{score:.3f}\", (x1, max(0,y1-6)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Show in notebook (RGB)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Faces: {len(dets)}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print details\n",
    "    print(\"Bounding boxes [x1,y1,x2,y2]:\", boxes)\n",
    "    for i, det in enumerate(dets):\n",
    "        info = {\"score\": float(det[4]) if len(det) > 4 else None}\n",
    "        if len(det) >= 15:\n",
    "            info[\"landmarks_5\"] = det[5:15].reshape(-1,2).tolist()\n",
    "        print(f\"Face {i+1}:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case the detector found multiple \"false positives\" in the background, but all of them have much lower confidence scores.\n",
    "\n",
    "Lets try to detect all available landmarks on a cropped face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"marc.jpg\"\n",
    "device = \"cpu\"  # or \"cuda\"\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# --- 1) detect first face\n",
    "det = FaceDetector(\"./weights/Alignment_RetinaFace.pth\", device=device)\n",
    "crop_face, dets = det.get_face(img_path)\n",
    "if len(dets)==0:\n",
    "    raise SystemExit(\"No faces detected.\")\n",
    "\n",
    "\n",
    "img_bgr = cv2.imread(img_path); h, w = img_bgr.shape[:2]\n",
    "x1,y1,x2,y2 = map(int, dets[0][:4]); x1,y1=max(0,x1),max(0,y1); x2,y2=min(w-1,x2),min(h-1,y2)\n",
    "crop = img_bgr[y1:y2, x1:x2].copy()\n",
    "\n",
    "# --- landmarks \n",
    "\n",
    "lmk = LandmarkDetector(\"./weights/Landmark_98.pkl\", device=device, device_ids=[0])\n",
    "\n",
    "\n",
    "pts = lmk.detect_landmarks(img_bgr, [dets[0]])[0].astype(np.float32)  # (98,2)\n",
    "\n",
    "# important landmark indices from WFLW ( https://wywu.github.io/projects/LAB/WFLW.html )\n",
    "important_idxs = {\n",
    "    16: (\"chin tip\", 16),\n",
    "    33: (\"left brow outer\", 33),\n",
    "    46: (\"right brow outer\", 46),\n",
    "    54: (\"nose tip\", 54),\n",
    "    60: (\"left eye outer\", 60),\n",
    "    72: (\"right eye outer\", 72),\n",
    "    76: (\"mouth left corner\", 76),\n",
    "    82: (\"mouth right corner\", 82),\n",
    "    79: (\"upper lip center\", 79),\n",
    "    85: (\"lower lip center\", 85),\n",
    "}\n",
    "\n",
    "# --- draw crop with landmarks\n",
    "vis = crop.copy()\n",
    "P = (pts - np.array([x1,y1])).astype(int)\n",
    "for (px,py) in P: \n",
    "    cv2.circle(vis, (px,py), 1, (0,255,255), -1)\n",
    "for k,(name,idx) in important_idxs.items():\n",
    "    cx,cy = int(pts[idx,0]-x1), int(pts[idx,1]-y1)\n",
    "    cv2.circle(vis, (cx,cy), 2, (0,0,255), -1)\n",
    "    cv2.putText(vis, str(k), (cx+4, cy-4), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0,0,0), 3, cv2.LINE_AA)\n",
    "    cv2.putText(vis, str(k), (cx+4, cy-4), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"First face with landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think that landmarks detected are accurate with what was described in WFLW dataset ( https://wywu.github.io/projects/LAB/WFLW.html )?\n",
    "\n",
    "If not, what could be the reason for this discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1.5: Out-of-the-box emotion recognition\n",
    "Now we may also try to detect emotions as well, using OpenFace. Do you agree with the model predictions? Test on different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- config\n",
    "image_path = \"marc.jpg\"\n",
    "device = \"cpu\"  # or \"cuda:0\"\n",
    "face_model_path = \"./weights/Alignment_RetinaFace.pth\"\n",
    "multitask_model_path = \"./weights/MTL_backbone.pth\"\n",
    "emo_names = [\"Neutral\",\"Happy\",\"Sad\",\"Surprise\",\"Fear\",\"Disgust\",\"Anger\",\"Contempt\"]\n",
    "\n",
    "# ---- init\n",
    "face_detector = FaceDetector(model_path=face_model_path, device=device)\n",
    "mtl = MultitaskPredictor(model_path=multitask_model_path, device=device)\n",
    "\n",
    "# ---- detect\n",
    "cropped_face, dets = face_detector.get_face(image_path)\n",
    "if cropped_face is None or dets is None or len(dets) == 0:\n",
    "    raise SystemExit(\"No face detected.\")\n",
    "\n",
    "img_bgr = cv2.imread(image_path)\n",
    "h, w = img_bgr.shape[:2]\n",
    "x1, y1, x2, y2 = map(int, dets[0][:4])\n",
    "x1, y1 = max(0, x1), max(0, y1)\n",
    "x2, y2 = min(w - 1, x2), min(h - 1, y2)\n",
    "\n",
    "# ---- multitask predict (per docs: returns emotion logits, gaze (yaw,pitch), AU intensities)\n",
    "emotion_logits, _, _ = mtl.predict(cropped_face)\n",
    "\n",
    "# emotion\n",
    "probs = F.softmax(emotion_logits, dim=1)[0].cpu().numpy()\n",
    "emo_idx = int(np.argmax(probs))\n",
    "emo_label = f\"{emo_names[emo_idx]} ({probs[emo_idx]*100:.1f}%)\"\n",
    "\n",
    "# ---- visualization\n",
    "vis = img_bgr.copy()\n",
    "# bbox\n",
    "cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# emotion label\n",
    "cv2.putText(vis, emo_label, (x1, max(0, y1 - 8)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "cv2.putText(vis, emo_label, (x1, max(0, y1 - 8)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ---- print raw details\n",
    "print(f\"Face bbox: [{x1},{y1},{x2},{y2}]\")\n",
    "print(f\"Emotion: idx={emo_idx} ({emo_names[emo_idx]}), probs={np.round(probs,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this block, attempt to join two parts of this Lab - camera access and facial/emotion detection. Write the code that will process live video stream, draw a bounding box around the face with the highest confidence score and write an emotion detected. Is it accurate enough?\n",
    "\n",
    "### Hint\n",
    "Openface-test python FaceDetector interface that we use ( https://github.com/CMU-MultiComp-Lab/OpenFace-3.0/tree/main ) reads images from file, so you may need to save frames into a temporary file. Ideally, this class should be modified to read data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2: Train an affect recognition system\n",
    "Traditional emotion recognition systems often employed a 2 stage pipeline:\n",
    "* Landmark extraction: from image extract useful landmarks (i.e., what you saw in S1.4)\n",
    "* Emotion classification: from landmarks detect a single emotion (i.e., what OpenFace implicitly did* in S1.5)\n",
    "\n",
    "More modern system can skip the landmark extraction phase and give you an emotional label directly.\n",
    "\n",
    "\\* OpenFace actually also uses other extracted features beside pure landmarks, but the point still stands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2.1\n",
    "Let's begin with the legacy approach. We will use OpenFace to extract landmarks from a set of images, then use those to train an emotion classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # for splitting data if needed\n",
    "from sklearn.svm import SVC  # simple classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report  # for evaluation\n",
    "from transformers import AutoImageProcessor, AutoModel  # for dino\n",
    "\n",
    "# using this to quickly load a random dataset. For the assignment and this lab, you will be provided with a specific dataset.\n",
    "# NOTE that this your current environment *does not have* this library as it is not required to do the project/assignment\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "det = FaceDetector(\"./weights/Alignment_RetinaFace.pth\", device=device)\n",
    "lmk = LandmarkDetector(\"./weights/Landmark_98.pkl\", device=device, device_ids=[0])\n",
    "\n",
    "# helper function to get landmarks in order to train a landmark->emotion model\n",
    "def detect_face_and_get_landmarks(image):\n",
    "    # transform PIL image to BGR numpy array\n",
    "    img_bgr = cv2.cvtColor(np.array(image.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    _, dets = det.get_face(img_bgr)\n",
    "    if dets is None or len(dets) == 0:\n",
    "        image.save(\"no_face_detected.png\")\n",
    "        return None\n",
    "\n",
    "    pts = lmk.detect_landmarks(img_bgr, [dets[0]])[0].astype(np.float32)  # (98,2)\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need an example dataset to train our model. For this lab we will use a random dataset. Note the you don't need to download this dataset as you will be provided with a different one for the lab exercises (and for the assignment).\n",
    "But, if you want to reproduce the results below, the can be downloaded with: \n",
    "```sh\n",
    "git clone https://huggingface.co/datasets/manojdilz/facial_emotion_detection_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"./facial_emotion_detection_dataset\")\n",
    "\n",
    "# unique classes, are already encoded in [0-6]\n",
    "classes = set(dataset[\"train\"][\"label\"])\n",
    "dataset_emo_mapping = [\"anger\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]  # 0 is anger, 1 is disgust, etc.\n",
    "\n",
    "# dataset is huge, we will sample 100 images per class\n",
    "images = []\n",
    "labels = []\n",
    "for cls in classes:\n",
    "    cls_images = dataset[\"train\"].filter(lambda x: x[\"label\"] == cls)\n",
    "    images.extend(cls_images[\"image\"][:100])\n",
    "    labels.extend(cls_images[\"label\"][:100])\n",
    "    \n",
    "# let's extract landmarks for all images\n",
    "landmark_features = []\n",
    "skipped = 0  # sometimes openface does not find a face (even though it's there), let's count how many\n",
    "skipped_indices = []  # used to filter no face images later\n",
    "for index, img in enumerate(images):\n",
    "    pts = detect_face_and_get_landmarks(img)\n",
    "    if pts is None:\n",
    "        skipped += 1\n",
    "        skipped_indices.append(index)\n",
    "        continue\n",
    "    feature_vector = pts.flatten()  # shape (196,)\n",
    "    landmark_features.append(feature_vector)\n",
    "\n",
    "print(f\"Skipped {skipped} images due to no face detected.\")\n",
    "\n",
    "images = [img for i, img in enumerate(images) if i not in skipped_indices]\n",
    "labels = [lbl for i, lbl in enumerate(labels) if i not in skipped_indices]\n",
    "\n",
    "# Let's begin training, we will use a simple SVM classifier\n",
    "svm = SVC()\n",
    "print(f\"Training SVM on {len(landmark_features)} samples...\")\n",
    "svm.fit(landmark_features, labels)\n",
    "print(\"SVM training completed.\")\n",
    "\n",
    "# Let's evaluate on the test set\n",
    "# test using dataset[\"test\"] up to 100 samples per class\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for cls in classes:\n",
    "    cls_images = dataset[\"test\"].filter(lambda x: x[\"label\"] == cls)\n",
    "    test_images.extend(cls_images[\"image\"][:100])\n",
    "    test_labels.extend(cls_images[\"label\"][:100])\n",
    "\n",
    "test_features = []\n",
    "skipped_test = 0\n",
    "skipped_test_indices = []\n",
    "for index, img in enumerate(test_images):\n",
    "    pts = detect_face_and_get_landmarks(img)\n",
    "    if pts is None:\n",
    "        skipped_test += 1\n",
    "        skipped_test_indices.append(index)\n",
    "        continue\n",
    "    feature_vector = pts.flatten()\n",
    "    test_features.append(feature_vector)\n",
    "print(f\"Skipped {skipped_test} test images due to no face detected.\")\n",
    "test_images = [img for i, img in enumerate(test_images) if i not in skipped_test_indices]\n",
    "test_labels = [lbl for i, lbl in enumerate(test_labels) if i not in skipped_test_indices]\n",
    "\n",
    "print(f\"Evaluating SVM on {len(test_features)} samples...\")\n",
    "preds = svm.predict(test_features)  # is an array of numeric labels e.g., [0,1,2,...]\n",
    "# let's replace numeric labels with string labels for better readability\n",
    "preds = [dataset_emo_mapping[p] for p in preds]\n",
    "test_labels_emo = [dataset_emo_mapping[t] for t in test_labels]\n",
    "acc = accuracy_score(test_labels_emo, preds)\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels_emo, preds, target_names=dataset_emo_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2.2: Comparison with OpenFace\n",
    "How does our super simple model compare to OpenFace's emotion detector? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_model_path = \"./weights/MTL_backbone.pth\"\n",
    "\n",
    "emo_names = [\"neutral\", \"happy\", \"sad\", \"surprise\", \"fear\", \"disgust\", \"anger\", \"contempt\"]\n",
    "\n",
    "mtl = MultitaskPredictor(model_path=multitask_model_path, device=device)\n",
    "\n",
    "\n",
    "def get_image_emotion(image):\n",
    "    img_bgr = cv2.cvtColor(np.array(image.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "    cropped_face, dets = det.get_face(img_bgr)\n",
    "    if cropped_face is None or dets is None or len(dets) == 0 or cropped_face.size == 0:\n",
    "        return None\n",
    "    emotion_logits, _, _ = mtl.predict(cropped_face)\n",
    "    probs = F.softmax(emotion_logits, dim=1)[0].cpu().numpy()\n",
    "    emo_idx = int(np.argmax(probs))\n",
    "    emo_label = f\"{emo_names[emo_idx]}\"\n",
    "    return emo_label\n",
    "\n",
    "\n",
    "preds = []\n",
    "for image in test_images:\n",
    "    emo = get_image_emotion(image)\n",
    "    if emo is None:\n",
    "        preds.append(\"\")  # invalid\n",
    "    else:\n",
    "        preds.append(emo)\n",
    "\n",
    "test_labels_emo = [dataset_emo_mapping[t] for t in test_labels]\n",
    "acc = accuracy_score(test_labels_emo, preds)\n",
    "print(f\"MTL Test Accuracy: {acc*100:.2f}%\")\n",
    "print(\"MTL Classification Report:\")\n",
    "print(classification_report(test_labels_emo, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2.3: Get emotions without landmarks\n",
    "\n",
    "Here, we are going to use DINOv3. We are going to feed it images and we will use its CLS token to extract the emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is DINO's smallest model. It is so small it can probably run on a calculator.\n",
    "pretrained_model_name = \"facebook/dinov3-vits16plus-pretrain-lvd1689m\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "model = AutoModel.from_pretrained(pretrained_model_name, device_map=\"auto\", dtype=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def extract_cls(images):\n",
    "    # get the cls token for the images\n",
    "    feats = []\n",
    "    for image in images:\n",
    "        inputs = processor(images=[image], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        pooled = outputs.pooler_output.detach().cpu()  # (B, D)\n",
    "        feats.append(pooled)\n",
    "    return torch.cat(feats, dim=0)\n",
    "\n",
    "train_in = extract_cls(images)\n",
    "svm_dino = SVC()\n",
    "print(f\"Training SVM on DINO features with {train_in.shape[0]} samples...\")\n",
    "svm_dino.fit(train_in.numpy(), labels)\n",
    "print(\"SVM training on DINO features completed.\")\n",
    "test_in = extract_cls(test_images)\n",
    "print(f\"Evaluating SVM on DINO features with {test_in.shape[0]} samples...\")\n",
    "preds_dino = svm_dino.predict(test_in.numpy())\n",
    "acc_dino = accuracy_score(test_labels, preds_dino)\n",
    "print(f\"DINO SVM Test Accuracy: {acc_dino*100:.2f}%\")\n",
    "print(\"DINO Classification Report:\")\n",
    "print(classification_report(test_labels, preds_dino, target_names=dataset_emo_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "With this notebook, you were provided with the MultiEmoVA dataset. Note that you are **not allowed** to use this dataset outside the scope of this course.\n",
    "\n",
    "The dataset contains pictures with multiple people in the frame. Use OpenFace's `FaceDetector` to extract the faces, then split your dataset into train and test. Use your train split to train a classifier (using either OpenFace's landmarks or DINO's approach). Finally, evaluate your model with accuracy and a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "Note that MultiEmoVA does not have emotions as labels, instead it uses valence/arousal (you should already be familiar with the difference from the lectures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
